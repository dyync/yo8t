training:
  model_name: "meta-llama/Llama-3-8b"
  output_dir: "./results"
  num_train_epochs: 3
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 2e-4
  fp16: true
  save_steps: 10
  logging_steps: 5
  max_seq_length: 512

lora:
  lora_alpha: 16
  lora_dropout: 0.1
  r: 64
  target_modules: ["q_proj", "v_proj"]
  task_type: "CAUSAL_LM"

evaluation:
  similarity_model: "all-MiniLM-L6-v2"
  min_rating_threshold: 7.0
  num_generations: 5
  temperature: 0.8
  max_new_tokens: 256

dataset:
  gold_standards_path: "./gold_standards.json"
  training_data_path: "./training_data.jsonl"